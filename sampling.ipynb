{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Overview\n",
    "\n",
    "Get the Detailed Overview: <a href=\"https://github.com/AnjulaMehto/Sampling_Assignment\">Here</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the overview of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0     0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1     0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2     1 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3     1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4     2 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      1  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('creditcard_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the dataset is balanced or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    763\n",
      "1      9\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_data_composition(df):\n",
    "    grouped_data = df.groupby(df['Class']).size().reset_index(name='count')\n",
    "    labels = grouped_data['Class'].astype(str)\n",
    "    sizes = grouped_data['count']\n",
    "    print(sizes)\n",
    "    colors = ['yellow', 'red']\n",
    "\n",
    "    plt.pie(sizes, labels=labels, colors=colors)\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "plot_data_composition(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see, our dataset is not at all balanced, so we will use resampling technique to balance the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Techniques:\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "        <b>Random sampling:</b> Systematic sampling involves selecting every kth item from a population, where k is a constant interval.\n",
    "    </li>\n",
    "    </br>\n",
    "    <li>\n",
    "        <b>Systematic sampling:</b> Systematic sampling involves selecting every kth item from a population, where k is a constant interval.\n",
    "    </li>\n",
    "    </br>\n",
    "    <li>\n",
    "        <b>Stratified sampling:</b> Duplicate instances from the minority class or generate synthetic samples to match the number of instances in the majority class.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for sample size calculation\n",
    "z_value = 2.576  # 99% confidence interval\n",
    "margin_error = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sample size for all sampling techniques using the formula\n",
    "sample_size = int(np.ceil((z_value**2 * 0.05 * (1-0.05)) / (margin_error**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling techniques and models\n",
    "samplers = {\n",
    "    'RandomUnderSampler': RandomUnderSampler(sampling_strategy='majority', random_state=40),\n",
    "    'RandomOverSampler': RandomOverSampler(sampling_strategy='minority', random_state=40),\n",
    "    'SMOTE': SMOTE(sampling_strategy='minority', random_state=40),\n",
    "    'NearMiss' : NearMiss(version=3, n_neighbors=4),\n",
    "    'SystematicSampling': 'systematic',  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=500),\n",
    "    'DecisionTree': DecisionTreeClassifier(random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'KNeighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a systematic sampling function\n",
    "def systematic_sampling(X, y, sample_size):\n",
    "    indexes = np.arange(0, len(X), len(X)//sample_size)\n",
    "    return X.iloc[indexes], y.iloc[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanyamgoyal401/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/sanyamgoyal401/.local/lib/python3.10/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/sanyamgoyal401/.local/lib/python3.10/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/sanyamgoyal401/.local/lib/python3.10/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/sanyamgoyal401/.local/lib/python3.10/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/sanyamgoyal401/.local/lib/python3.10/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/sanyamgoyal401/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/sanyamgoyal401/.local/lib/python3.10/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/sanyamgoyal401/.local/lib/python3.10/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/sanyamgoyal401/.local/lib/python3.10/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/sanyamgoyal401/.local/lib/python3.10/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/sanyamgoyal401/.local/lib/python3.10/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/sanyamgoyal401/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/sanyamgoyal401/.local/lib/python3.10/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/sanyamgoyal401/.local/lib/python3.10/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/sanyamgoyal401/.local/lib/python3.10/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/sanyamgoyal401/.local/lib/python3.10/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/sanyamgoyal401/.local/lib/python3.10/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/sanyamgoyal401/.local/lib/python3.10/site-packages/imblearn/under_sampling/_prototype_selection/_nearmiss.py:203: UserWarning: The number of the samples to be selected is larger than the number of samples available. The balancing ratio cannot be ensure and all samples will be returned.\n",
      "  warnings.warn(\n",
      "/home/sanyamgoyal401/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/sanyamgoyal401/.local/lib/python3.10/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/sanyamgoyal401/.local/lib/python3.10/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/sanyamgoyal401/.local/lib/python3.10/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/sanyamgoyal401/.local/lib/python3.10/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/sanyamgoyal401/.local/lib/python3.10/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/sanyamgoyal401/.local/lib/python3.10/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/sanyamgoyal401/.local/lib/python3.10/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/sanyamgoyal401/.local/lib/python3.10/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/sanyamgoyal401/.local/lib/python3.10/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/sanyamgoyal401/.local/lib/python3.10/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Evaluate each model on each sampling technique\n",
    "results = {}\n",
    "for sampler_name, sampler in samplers.items():\n",
    "    if sampler_name == 'SystematicSampling':\n",
    "        X_resampled, y_resampled = systematic_sampling(X_train, y_train, sample_size)\n",
    "    else:\n",
    "        X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n",
    "    for model_name, model in models.items():\n",
    "        # Train the model on the resampled data\n",
    "        model.fit(X_resampled, y_resampled)\n",
    "\n",
    "        # Ensure X_test is a NumPy array and C-contiguous\n",
    "        X_test_array = np.ascontiguousarray(np.array(X_test))\n",
    "\n",
    "        # Make predictions on the test data\n",
    "        y_pred = model.predict(X_test_array)\n",
    "\n",
    "        # Calculate the accuracy score\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # Add the accuracy score to the results dictionary\n",
    "        results.setdefault(model_name, {}).setdefault(sampler_name, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "                    RandomUnderSampler  RandomOverSampler   SMOTE               NearMiss            SystematicSampling  \n",
      "LogisticRegression  0.91885             0.92670             0.92147             0.71466             0.87435             \n",
      "DecisionTree        0.99738             0.99738             0.99738             0.89791             0.95288             \n",
      "RandomForest        1.00000             1.00000             1.00000             0.92147             0.99215             \n",
      "SVM                 0.69895             0.70157             0.69895             0.51571             0.70942             \n",
      "KNeighbors          0.98168             0.98429             0.98429             0.74869             0.87435             \n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print('Results:')\n",
    "print(' ' * 20, end='')  # Padding\n",
    "for sampler_name in samplers.keys():\n",
    "    print(f'{sampler_name:<20}', end='')\n",
    "print()\n",
    "\n",
    "for model_name, model_results in results.items():\n",
    "    print(f'{model_name:<20}', end='')\n",
    "    for sampler_name in samplers.keys():\n",
    "        print(f'{results[model_name].get(sampler_name, 0.0):<17.5f}   ', end='')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
